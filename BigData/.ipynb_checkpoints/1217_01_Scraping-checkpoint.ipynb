{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 웹크로울링 :스크렙핑 반복 <-> 웹스크렙핑 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 웹스크렙핑1 : 기본"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사전작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as req"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이미지 (url -> download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save_path1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-11191164b397>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msave_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'd:/test1.jpg'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# 이미지를 웹에서 검색해서 다운받음 (다운경로, 저장위치)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\n\\n\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'다운로드 성공!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'save_path1' is not defined"
     ]
    }
   ],
   "source": [
    "img_url ='http://cafefiles.naver.net/20130213_215/cnclover_13607476765959Qg35_JPEG/%C0%BD%B8%F0%B8%A6_%B2%D9%B9%CE_%B0%ED%BE%E7%C0%CC.jpg'\n",
    "save_path = 'd:/test1.jpg'\n",
    "\n",
    "file, header = req.urlretrieve(img_url, save_path1)# 이미지를 웹에서 검색해서 다운받음 (다운경로, 저장위치)\n",
    "print(file, '\\n\\n\\n', header, '다운로드 성공!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_url=\"http://google.com\"\n",
    "save_path = 'd:/index.html'\n",
    "\n",
    "file, header = req.urlretrieve(html_url, save_path)\n",
    "print(file1, '\\n\\n\\n',header1,'다운로드 성공!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 오류처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_url=\"http://google.com\"\n",
    "save_path = 'd:/index.html'\n",
    "\n",
    "try:\n",
    "    file, header = req.urlretrieve(html_url, save_path)\n",
    "except Exception as err:\n",
    "    print('다운로드 실패!')\n",
    "    print(err)\n",
    "else :\n",
    "    print(file1, '\\n\\n\\n', header,'다운로드 성공!')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 웹스크렙핑2 : 여러 링크"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사전작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request as req\n",
    "from urllib.error import URLError, HTTPError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> URLError : 잘못된 주소를 요청한 경우 <br>\n",
    "  HTTPErorr : 서버가 접속이 안됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 링크배열 => 여러 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list =['d:/index.html','d:/test1.jpg']\n",
    "target_url = [\"http://google.com\", 'http://cafefiles.naver.net/20130213_215/cnclover_13607476765959Qg35_JPEG/%C0%BD%B8%F0%B8%A6_%B2%D9%B9%CE_%B0%ED%BE%E7%C0%CC.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for i, url in enumerate(target_url):\n",
    "        res = req.urlopen(url) \n",
    "        contents = res.read()\n",
    "        with open(path_list[i], 'wb') as f:\n",
    "            f.write(contents)\n",
    "except HTTPError as err:\n",
    "    print('HTTPError Code :', err.code)\n",
    "except URLError as err:\n",
    "    print('URLError Code :', err.reason)\n",
    "else :\n",
    "    print('다운로드 성공!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 웹스크렙핑3 : lxml모듈, lxml.cssselect, 네이버 메인의 신문사 url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml.html\n",
    "import cssselect # pip install cssselect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Url, Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_news_list_page(response) :\n",
    "    urls =[]\n",
    "    root = lxml.html.fromstring(response.content)\n",
    "    for a in root.cssselect('.api_list .api_item a.api_link'): # id : #, class : ., api_list아래 .api_item아래\n",
    "        urls.append([a[0].get('alt'), a.get('href')])\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('https://www.naver.com')\n",
    "urls = scrap_news_list_page(res)\n",
    "\n",
    "for items in urls :\n",
    "    print()\n",
    "    for item in items :\n",
    "        print(item, end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 선생님"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_news_list_page_dic(response) :\n",
    "    urls ={}\n",
    "    root = lxml.html.fromstring(response.content)\n",
    "    for a in root.cssselect('.api_list .api_item a.api_link'): # id : #, class : ., api_list아래 .api_item아래\n",
    "        urls[a[0].get('alt')] = a.get('href')\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-98a9f4748a14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://www.naver.com'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0murls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrap_news_list_page_dic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "res = requests.get('https://www.naver.com')\n",
    "urls = scrap_news_list_page_dic(res)\n",
    "\n",
    "for name, url in urls.items() :\n",
    "    print(name, url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 웹스크렙핑4 : lxml.xpath 이용, 네이버 메인의 신문사 url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /html/body/div[3]/div[3]/div[1]/div[2]/div[2]/div[2]/div/div/div/div/ul/li[4]\n",
    "# //*[@id=\"NS_081\"]\n",
    "\n",
    "def scrap_news_list_page_dic(response) :\n",
    "    urls ={}\n",
    "    root = lxml.html.fromstring(response.content)\n",
    "    for a in root.xpath('//ul[@class=\"api_list\"]/li[@class=\"api_item\"]/a[@class=\"api_link\"]'): # //태그[@class=\"클래스명\"]/태그[@class=\"클래스명\"] \n",
    "        urls[a[0].get('alt')] = a.get('href')\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res = requests.get('https://www.naver.com')\n",
    "urls = scrap_news_list_page_dic(res)\n",
    "\n",
    "for name, url in urls.items() :\n",
    "    print(name, url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 웹스크렙핑5 : Get RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = 'https://www.mois.go.kr/gpms/view/jsp/rss/rss.jsp'\n",
    "params = []\n",
    "\n",
    "for num in [1001, 1012, 1013, 1014]:\n",
    "    params.append(dict(ctxCd=num)) \n",
    "\n",
    "for c in params:\n",
    "    param = urllib.parse.urlencode(c)\n",
    "    url = api + '?' + param\n",
    "    res_data = urllib.request.urlopen(url).read()\n",
    "    print(res_data.decode('utf-8'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 웹스크렙핑6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as req"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인기검색어 가져오기 X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url =\"https://finance.daum.net/\"\n",
    "\n",
    "res = req.urlopen(req.Request(url)).read().decode('utf-8')\n",
    "print(res)\n",
    "\n",
    "# 인기검색어 x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인기검색어 가져오기 : Forbidden 우회"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> fake-useragent 모듈 이용<br>\n",
    "  pip install fake-useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://finance.daum.net/api/search/ranks?limit=10'\n",
    "res_data = urllib.request.urlopen(url).read()\n",
    "print(res_data.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fake_useragent import UserAgent\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ua = UserAgent()\n",
    "headers = {\n",
    "    'User-Agent' : ua.ie,\n",
    "    'referer' : 'https://finance.daum.net/'\n",
    "}\n",
    "\n",
    "url = 'https://finance.daum.net/api/search/ranks?limit=10'\n",
    "res = req.urlopen(req.Request(url, headers=headers)).read().decode('utf-8')\n",
    "rank_json = json.loads(res)['data']\n",
    "\n",
    "for elem in rank_json:\n",
    "    print(elem['rank'], elem['name'], elem['tradePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 웹스크렙핑6 : Beautiful Soup\n",
    "> * css나 xpath 쉽게 사용<br>\n",
    "* html 파싱시 사용<br>\n",
    "* 내부적으로 lxml포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html='''\n",
    "<html><head><title>The Dormouse's story</title></head><body><h1>this is h1 area</h1><h2>this is h2 area</h2><p class=\"title\"><b>The Dormouse's story</b></p><p class=\"story\">Once upon a time there were three little sisters<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a><a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a><a data-io=\"link3\" href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a></p><p class=\"story\">story...</p></body></html>\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup 객체 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html,'html.parser') # 뷰티풀숩 객체 초기화\n",
    "print(type(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### html파일 예쁘게 정렬후 보여주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify()) # 예쁘게 보여주기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 태그읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h1 = soup.html.body.h1\n",
    "print(h1 ,',', h1.string, ' ,',h1.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = soup.html.body.p\n",
    "print(p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 형재관계 태그읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = soup.html.body.p\n",
    "print(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = p1.next_sibling\n",
    "print(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p3 = p2.next_sibling\n",
    "print(p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(list(p2.next_elements), '\\n\\n')\n",
    "for item in p2.next_elements:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 특정 태그 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "links1 =soup.find_all('a')\n",
    "print(links1, '\\n')\n",
    "\n",
    "links2 =soup.find('a')\n",
    "print(links2, '\\n')\n",
    "\n",
    "links3 =soup.find('a', {\"class\":\"sister\", \"data-io\":\"link3\"})\n",
    "print(links3, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select() select_one()\n",
    "link1 = soup.select_one('p.title > b')\n",
    "\n",
    "link2 = soup.select_one('a#link1')\n",
    "\n",
    "link3 =soup.select_one('a[data-io=\"lihk3\"]')\n",
    "\n",
    "print(link1.text, link2.text, link3.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link4 = soup.select('p.story > a')\n",
    "\n",
    "print(link4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 웹스크렙핑7 : Get Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as req\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folrder is created!\n"
     ]
    }
   ],
   "source": [
    "savePath = 'C:/Users/GD6/imagedown/'\n",
    "try:\n",
    "    if not(os.path.isdir(savePath)):\n",
    "        os.makedirs(os.path.join(savePath))\n",
    "except OSError as e:\n",
    "    print('forder createion failed!')\n",
    "    raise RuntimeError('SystemExit')\n",
    "else:\n",
    "    print('folrder is created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 'https://search.naver.com/search.naver?sm=tab_hty.top&where=image&query='\n",
    "quote = urllib.parse.quote_plus('갈치')\n",
    "\n",
    "url = base+quote\n",
    "# print(url)\n",
    "res = req.urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "img_list = soup.select(\"div.photo_grid > div.img_area > a.thumb._thumb > img\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다운로드 성공!\n"
     ]
    }
   ],
   "source": [
    "for i, img_list in enumerate(img_list, 1):\n",
    "    fullFileName = os.path.join(savePath, savePath + str(i)+ '.png')\n",
    "    req.urlretrieve(img_list['data-source'], fullFileName)\n",
    "print('다운로드 성공!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
